
# [Hello](/download/cover.pdf)

Devops engineer with a bachelor's degree in CS and near the five years of
experience in some technologies and programming languages.
I mainly act in a devops field: from automation of routine processes using
scripting languages like Python and Bash to develop and maintain reliable
systems for big data processing in cloud platforms (in particular AWS).
I also have a great interest in systems programming: especially the development
in Rust.

Always try to keep my level of awareness of technologies up to date, hence I
periodically read books about old but super important stuff (Linux API, data
structures, algorithms etc) and new projects as well (Docker, Rust etc). Also
aspire to contribute to open source software one day.

From the software development perspective I try to grab hands on experience by
develop pet projects (please check my [github](https://github.com/enkron)
account for examples).  Currently I'm digging into the type system, data
structures and references' lifetimes. And plan to integrate concurrent/async
programming into my daily activities.

Started as a support engineer in a big data field.
Our team was responsible for keeping the distributed data processing
clusters up and running (Hadoop, Hive, Impala, Spark, HBase, Zookeper, Apache
Kafka). Also this position provided me an opportunity to learn Linux internals
more deeply (Linux PAM, cgroups, systemd, LVM etc).
As a operation team member I was also involved into communication with the
customers.

After promotion to the Devops engineer role I was developing CI/CD pipelines
for customers' applications which exploited big data platform using Jenkins,
Ansible, Python, Gitlab and Bash.

Modeled, developed and maintained AWS CloudFormation templates for managing and
provisioning infrastructure as a code.

Refactored and evolved existing Ansible playbooks for full infrastructure
configuration.

Configured Kerberos security protocol for authentication of services within
Hadoop cluster.

Automated standard operations and procedures to reduce manual work and time for
routines (using Jenkins scripted pipelines and Python / Bash scripts)

Provided backup and disaster recovery strategies for infrastructure (with AWS
capabilities: EBS snapshots, RDS multi A-Z, S3 cross-region replication etc).

Containerized custom application for big data processing using Docker. There
was a request for storing all components in a single container, that's
absolutely contradicts Docker's paradigm. Nevertheless build process included
configuration of the application itself and provisioning and configuration
database (Postgresql). The total weight of the container was quite large (~4.29
GB) and our team planned to split the application into separate services but
unfortunately I didn't see the final result as I finished working for the
company by that time.

In my last work experience I dealt with bare-metal servers rather than the now
familiar cloud solutions. To automate routine processes I was creating Ansible
playbooks for upgrade and deployment Confluence instances to the stage and
production environments. Also was deploying and troubleshooting Bitbucket's
mirror farm.
Our team has created CI/CD pipelines using Buildbot (not very popular
solution). I've also developed CLI utility using Python for automatic migration
from one CRM system to another (Salesforce to JIRA) in a fairly short period of
time. And of course my daily duties included monitoring the state of the
servers, administering and configuring Linux hosts (authentication, logging,
cleanup, provisioning new VMs and containers, networking problems etc),
administering Atlassian stack (JIRA, Confluence, Bitbucket).

Many thanks,<br>
Sergei
